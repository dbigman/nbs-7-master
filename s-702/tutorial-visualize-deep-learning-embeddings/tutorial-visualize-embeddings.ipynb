{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing word embeddings : Deep Learning for NLP embeddings.\n",
    "\n",
    "This notebook will guide you through the implementation of Word2Vec, a popular model used in Natural Language Processing (NLP) to capture semantic meaning from text. We will apply this model to a dataset and visualize the learned word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/deep_learning.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "In this notebook, we will apply the Word2Vec model to a text dataset. The Word2Vec model learns to represent words in a continuous vector space, where semantically similar words are mapped to nearby points. We will walk through the following steps:\n",
    "\n",
    "1. **Download the Training Data:** We'll start by downloading a large text corpus for training.\n",
    "2. **Set up Word2Vec in TensorFlow:** We'll configure the Word2Vec model using TensorFlow, a popular machine learning library.\n",
    "3. **Train the Model:** We'll train the Word2Vec model on the dataset to learn word embeddings.\n",
    "4. **Visualize the Embeddings:** Finally, we'll visualize the learned word embeddings using dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I like to live dangerously ‚ò†\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "import random\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download the Dataset\n",
    "\n",
    "To train our Word2Vec model, we need a large text corpus. In this step, we will download the \"text8\" dataset, a popular dataset used for training word embeddings. This dataset contains 100 MB of English Wikipedia text from 2006, which is commonly used in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "# def maybe_download(path, filename, expected_bytes):\n",
    "#     \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "#     if not os.path.exists(path+filename):\n",
    "#         filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    \n",
    "#     statinfo = os.stat(path+filename)\n",
    "#     if statinfo.st_size == expected_bytes:\n",
    "#         print('Found and verified', filename)\n",
    "#     else:\n",
    "#         print(statinfo.st_size)\n",
    "#         raise Exception(\n",
    "#             'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    \n",
    "#     return filename\n",
    "\n",
    "# path = \"\"\n",
    "# filename = maybe_download(path, 'text8.zip', 31_344_016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def verify_files(path, filenames):\n",
    "    missing_files = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(filename)\n",
    "        else:\n",
    "            statinfo = os.stat(file_path)\n",
    "            if statinfo.st_size == 0:\n",
    "                empty_files.append(filename)\n",
    "                \n",
    "    if missing_files:\n",
    "        raise Exception(f\"Missing files: {', '.join(missing_files)}\")\n",
    "    if empty_files:\n",
    "        raise Exception(f\"Empty files: {', '.join(empty_files)}\")\n",
    "    \n",
    "    print(\"All files found and verified!\")\n",
    "\n",
    "# Path to your local dataset\n",
    "path = \"../datasets/harry_potter_books\"  # Change this to your actual path\n",
    "\n",
    "# List of books to check\n",
    "harry_potter_books = [\n",
    "    \"Book 1 - The Philosopher's Stone.txt\",\n",
    "    \"Book 2 - The Chamber of Secrets.txt\",\n",
    "    \"Book 3 - The Prisoner of Azkaban.txt\",\n",
    "    \"Book 4 - The Goblet of Fire.txt\",\n",
    "    \"Book 5 - The Order of the Phoenix.txt\",\n",
    "    \"Book 6 - The Half Blood Prince.txt\",\n",
    "    \"Book 7 - The Deathly Hallows.txt\"\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "verify_files(path, harry_potter_books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** While the dataset is being downloaded, feel free to look ahead at the upcoming code cells to get a sense of what we'll be doing next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assumes you're reading plain text files directly\n",
    "def read_data(path, filenames):\n",
    "    \"\"\"Read multiple files from a given path and compile their contents into a list of words.\"\"\"\n",
    "    vocabulary = []  # Initialize an empty list to hold all words\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "            # Read the file's content, split into words, and extend the vocabulary list\n",
    "            words = file.read().split()\n",
    "            vocabulary.extend(words)\n",
    "    return vocabulary\n",
    "\n",
    "# Assuming 'path' is already defined as you showed earlier\n",
    "# And 'harry_potter_books' is your list of filenames\n",
    "vocabulary = read_data(path, harry_potter_books)\n",
    "print('Dataset size: {:,} words'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a peak at the head\n",
    "vocabulary[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Preprocessing\n",
    "\n",
    "- **Case and Punctuation:** Notice that none of the words in our dataset are capitalized, and there is no punctuation. This is likely due to preprocessing steps taken before we acquired the dataset, which ensures uniformity in the text.\n",
    "- **Preprocessing Considerations:** Preprocessing is a crucial part of NLP tasks, and its specifics depend on the data and the intended use. For instance, while we're working with unigrams (single words), other tasks might benefit from encoding n-grams (combinations of words) to capture context better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def read_and_preprocess_data(directory):\n",
    "    \"\"\"Reads multiple text files from a directory and preprocesses the text.\"\"\"\n",
    "    words = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory)):  # Ensure order (HP1 ‚Üí HP7)\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.endswith(\".txt\"):  # Process only text files\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = simple_preprocess(text, deacc=True)  # Tokenize & remove punctuation\n",
    "                words.extend(tokens)  # Add to vocabulary\n",
    "\n",
    "    return words\n",
    "\n",
    "# Define dataset directory (relative path)\n",
    "dataset_path = \"../datasets/harry_potter_books\"\n",
    "\n",
    "# Read and preprocess the dataset\n",
    "vocabulary = read_and_preprocess_data(dataset_path)\n",
    "\n",
    "print('Dataset size after preprocessing: {:,} words'.format(len(vocabulary)))\n",
    "print('Sample words:', vocabulary[:50])  # Print a preview of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "def train_word2vec(vocabulary):\n",
    "    \"\"\"Trains a Word2Vec model on the processed Harry Potter dataset.\"\"\"\n",
    "    \n",
    "    # Word2Vec expects sentences (list of lists), so we wrap words in one list\n",
    "    sentences = [vocabulary]\n",
    "    \n",
    "    # Train model using Skip-Gram (sg=1), size=100 (vector dimensions), window=5 (context size)\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train Word2Vec on the Harry Potter dataset\n",
    "w2v_model = train_word2vec(vocabulary)\n",
    "\n",
    "# Save model for later use\n",
    "w2v_model.save(\"harry_potter_word2vec.model\")\n",
    "\n",
    "print(\"Word2Vec model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "\n",
    "# Find similar words to \"harry\"\n",
    "print(\"Most similar words to 'harry':\")\n",
    "print(model.wv.most_similar(\"harry\", topn=5))\n",
    "\n",
    "# Find similar words to \"magic\"\n",
    "print(\"\\nMost similar words to 'magic':\")\n",
    "print(model.wv.most_similar(\"magic\", topn=5))\n",
    "\n",
    "# Find similarity between two words\n",
    "print(\"\\nSimilarity between 'harry' and 'voldemort':\", model.wv.similarity(\"harry\", \"voldemort\"))\n",
    "print(\"Similarity between 'hogwarts' and 'castle':\", model.wv.similarity(\"hogwarts\", \"castle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load trained Word2Vec model\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "\n",
    "def plot_tsne(model, num_words=100):\n",
    "    \"\"\"Reduces word embeddings to 2D using t-SNE and plots them.\"\"\"\n",
    "    \n",
    "    # Select most common words in the vocabulary\n",
    "    words = list(model.wv.index_to_key)[:num_words]\n",
    "    \n",
    "    # Get corresponding word vectors\n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Reduce dimensions using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    # Plot the words in 2D space\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], alpha=0.7)\n",
    "\n",
    "    # Annotate points with words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=9)\n",
    "\n",
    "    plt.title(\"t-SNE visualization of Word2Vec embeddings (Harry Potter)\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize embeddings\n",
    "plot_tsne(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load trained Word2Vec model\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "\n",
    "def cluster_words(model, num_clusters=5):\n",
    "    \"\"\"Clusters words into groups using K-Means.\"\"\"\n",
    "    \n",
    "    words = list(model.wv.index_to_key)[:500]  # Select top 500 words\n",
    "    word_vectors = np.array([model.wv[word] for word in words])  # Get word embeddings\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "    # Group words by cluster\n",
    "    clustered_words = {i: [] for i in range(num_clusters)}\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        clustered_words[cluster].append(word)\n",
    "\n",
    "    return clustered_words\n",
    "\n",
    "# Get clustered words\n",
    "num_clusters = 5  # Choose number of clusters\n",
    "clusters = cluster_words(model, num_clusters)\n",
    "\n",
    "# Print clusters\n",
    "for cluster, words in clusters.items():\n",
    "    print(f\"\\nüü¢ Cluster {cluster+1}:\")\n",
    "    print(\", \".join(words[:15]))  # Show top words per cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(word1, word2, word3, model):\n",
    "    \"\"\"Solves word analogies: word1 - word2 + word3 = ?\"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(positive=[word1, word3], negative=[word2], topn=1)\n",
    "        return result[0][0]  # Return the best match\n",
    "    except KeyError as e:\n",
    "        return str(e)  # Handle missing words\n",
    "\n",
    "# Example analogies\n",
    "print(\"\\nüîÑ Word Analogies:\")\n",
    "print(f\"'harry' - 'gryffindor' + 'slytherin' = {find_analogies('harry', 'gryffindor', 'slytherin', model)}\")\n",
    "print(f\"'dumbledore' - 'good' + 'evil' = {find_analogies('dumbledore', 'good', 'evil', model)}\")\n",
    "print(f\"'wand' - 'magic' + 'weapon' = {find_analogies('wand', 'magic', 'weapon', model)}\")\n",
    "print(f\"'quidditch' - 'sport' + 'game' = {find_analogies('quidditch', 'sport', 'game', model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load trained Word2Vec model\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "\n",
    "def plot_clusters_tsne(model, num_clusters=5):\n",
    "    \"\"\"Plots word embeddings using t-SNE and colors by clusters.\"\"\"\n",
    "    \n",
    "    words = list(model.wv.index_to_key)[:500]  # Select top words\n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Reduce dimensions using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    # Cluster words using K-Means\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "    # Plot words with cluster colors\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # Annotate words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=8)\n",
    "\n",
    "    plt.title(\"t-SNE Visualization of Word Clusters (Harry Potter)\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "plot_clusters_tsne(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Step 2: Build the dictionary \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Example dataset: Sentences classified into Hogwarts houses\n",
    "sentences = [\n",
    "    (\"Bravery and courage are the most important virtues.\", \"Gryffindor\"),\n",
    "    (\"Books and learning are what truly matter.\", \"Ravenclaw\"),\n",
    "    (\"Cunning and ambition will take you far.\", \"Slytherin\"),\n",
    "    (\"Loyalty and hard work define us.\", \"Hufflepuff\"),\n",
    "    (\"We must fight for what is right.\", \"Gryffindor\"),\n",
    "    (\"Intelligence and wisdom are our strengths.\", \"Ravenclaw\"),\n",
    "    (\"Power is the path to greatness.\", \"Slytherin\"),\n",
    "    (\"Kindness and dedication make a difference.\", \"Hufflepuff\"),\n",
    "]\n",
    "\n",
    "# Convert sentences to vectors\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence, label in sentences:\n",
    "    words = sentence.lower().split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    sentence_vector = np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
    "    X.append(sentence_vector)\n",
    "    y.append(label)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "sample_sentence = \"I want to be professor of magic\"\n",
    "sample_vector = np.mean([model.wv[word] for word in sample_sentence.lower().split() if word in model.wv], axis=0)\n",
    "predicted_class = clf.predict([sample_vector])\n",
    "\n",
    "print(f\"\\nüìö Sentence: '{sample_sentence}' ‚Üí Predicted House: {predicted_class[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Example knowledge base\n",
    "responses = {\n",
    "    \"harry\": [\"Harry is the Boy Who Lived.\", \"Harry Potter is a Gryffindor.\", \"Harry faced Voldemort many times.\"],\n",
    "    \"hermione\": [\"Hermione is the brightest witch of her age.\", \"Hermione loves books and learning.\"],\n",
    "    \"ron\": [\"Ron Weasley is Harry's best friend.\", \"Ron comes from a big wizarding family.\"],\n",
    "    \"hogwarts\": [\"Hogwarts is the best magical school.\", \"Hogwarts has four houses: Gryffindor, Slytherin, Ravenclaw, and Hufflepuff.\"],\n",
    "    \"spell\": [\"Expecto Patronum is a powerful spell.\", \"Avada Kedavra is an unforgivable curse.\"]\n",
    "}\n",
    "\n",
    "# Load trained Word2Vec model\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "logging.info(\"Word2Vec model loaded successfully.\")\n",
    "\n",
    "import random\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Example knowledge base\n",
    "responses = {\n",
    "    \"harry\": [\"Harry is the Boy Who Lived.\", \"Harry Potter is a Gryffindor.\", \"Harry faced Voldemort many times.\"],\n",
    "    \"hermione\": [\"Hermione is the brightest witch of her age.\", \"Hermione loves books and learning.\"],\n",
    "    \"ron\": [\"Ron Weasley is Harry's best friend.\", \"Ron comes from a big wizarding family.\"],\n",
    "    \"hogwarts\": [\"Hogwarts is the best magical school.\", \"Hogwarts has four houses: Gryffindor, Slytherin, Ravenclaw, and Hufflepuff.\"],\n",
    "    \"spell\": [\"Expecto Patronum is a powerful spell.\", \"Avada Kedavra is an unforgivable curse.\"]\n",
    "}\n",
    "\n",
    "# Load trained Word2Vec model\n",
    "model = Word2Vec.load(\"harry_potter_word2vec.model\")\n",
    "logging.info(\"Word2Vec model loaded successfully.\")\n",
    "\n",
    "def chatbot_response(user_input, model):\n",
    "    \"\"\"Finds a suitable response using exact matches, phrase handling, or embeddings with logging.\"\"\"\n",
    "    if not user_input.strip():  # Handle empty input\n",
    "        logging.warning(\"User provided empty input.\")\n",
    "        return \"I didn't catch that. Could you try asking again?\"\n",
    "\n",
    "    logging.info(f\"User input: {user_input}\")\n",
    "    words = user_input.lower().split()\n",
    "\n",
    "    # Step 1: Check for exact matches for multi-word phrases\n",
    "    if user_input.lower() in responses:\n",
    "        response = random.choice(responses[user_input.lower()])\n",
    "        logging.info(f\"Exact match found for phrase '{user_input}': {response}\")\n",
    "        return response\n",
    "\n",
    "    # Step 2: Check for exact matches for individual words\n",
    "    for word in words:\n",
    "        if word in responses:\n",
    "            response = random.choice(responses[word])\n",
    "            logging.info(f\"Exact match found for '{word}': {response}\")\n",
    "            return response\n",
    "\n",
    "    # Step 3: Use Word2Vec to find similar words, with a similarity threshold\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            logging.info(f\"Word '{word}' found in embeddings.\")\n",
    "            try:\n",
    "                # Get the most similar word and its similarity score\n",
    "                similar_word, similarity = model.wv.most_similar(word, topn=1)[0]\n",
    "                logging.info(f\"Most similar word to '{word}': {similar_word} (similarity: {similarity:.2f})\")\n",
    "\n",
    "                # Use the similar word only if the similarity is above a threshold\n",
    "                if similarity > 0.5 and similar_word in responses:\n",
    "                    response = random.choice(responses[similar_word])\n",
    "                    logging.info(f\"Response chosen for similar word '{similar_word}': {response}\")\n",
    "                    return response\n",
    "                else:\n",
    "                    logging.warning(f\"Similar word '{similar_word}' ignored due to low similarity ({similarity:.2f}).\")\n",
    "            except KeyError as e:\n",
    "                logging.warning(f\"KeyError for word '{word}': {e}\")\n",
    "\n",
    "    # Step 4: Fallback response\n",
    "    logging.warning(\"No suitable response found. Falling back to default.\")\n",
    "    return \"I don't know much about that. Maybe ask about Hogwarts or spells?\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        logging.info(\"User exited the chatbot.\")\n",
    "        print(\"Chatbot: Goodbye, and may the magic be with you! ‚ú®\")\n",
    "        break\n",
    "    response = chatbot_response(user_input, model)\n",
    "    print(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Harry\n",
      "Potter\n",
      "is\n",
      "a\n",
      "wizard\n",
      "who\n",
      "studies\n",
      "at\n",
      "Hogwarts\n",
      ".\n",
      "\n",
      "Named Entities:\n",
      "Harry Potter (PERSON)\n",
      "Hogwarts (GPE)\n",
      "\n",
      "Part-of-Speech Tags:\n",
      "Harry - PROPN\n",
      "Potter - PROPN\n",
      "is - AUX\n",
      "a - DET\n",
      "wizard - NOUN\n",
      "who - PRON\n",
      "studies - VERB\n",
      "at - ADP\n",
      "Hogwarts - PROPN\n",
      ". - PUNCT\n",
      "\n",
      "Dependency Parsing:\n",
      "Harry -> compound -> Potter\n",
      "Potter -> nsubj -> is\n",
      "is -> ROOT -> is\n",
      "a -> det -> wizard\n",
      "wizard -> attr -> is\n",
      "who -> nsubj -> studies\n",
      "studies -> relcl -> wizard\n",
      "at -> prep -> studies\n",
      "Hogwarts -> pobj -> at\n",
      ". -> punct -> is\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process some text\n",
    "text = \"Harry Potter is a wizard who studies at Hogwarts.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization: Break text into words\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# Named Entity Recognition (NER): Identify entities in text\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "# Part-of-Speech (POS) tagging\n",
    "print(\"\\nPart-of-Speech Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.pos_}\")\n",
    "\n",
    "# Dependency Parsing: Relationships between words\n",
    "print(\"\\nDependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Sample Harry Potter text\n",
    "text = \"\"\"\n",
    "Harry Potter and Hermione Granger walked through the halls of Hogwarts. \n",
    "They were looking for Professor Dumbledore in his office. Meanwhile, \n",
    "Lord Voldemort was plotting an attack on the Ministry of Magic.\n",
    "\"\"\"\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities related to characters & places\n",
    "characters = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "places = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"ORG\", \"LOC\"]]\n",
    "\n",
    "# Print extracted results\n",
    "print(\"üßô Characters:\", set(characters))\n",
    "print(\"üìç Places:\", set(places))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we have our text data, the next step is to build a dictionary. This dictionary will map each unique word in our dataset to a unique integer code. \n",
    "\n",
    "Additionally, we'll keep track of the frequency of each word, which will help us prioritize the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50_000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding the Key Variables\n",
    "\n",
    "- **`data`:** A list of integer codes representing the words in the original text. Each word is replaced by its corresponding code from the `dictionary`.\n",
    "- **`count`:** A list of tuples where each tuple contains a word and its frequency count in the dataset.\n",
    "- **`dictionary`:** A dictionary mapping each word to its unique integer code.\n",
    "- **`reverse_dictionary`:** A reverse mapping that converts integer codes back to their corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary # Reduce memory by getting rid of the \"heavy\" list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[:5] # An index of each word (as it appears in order) to its rank. Therefore we don't have reference the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary['the'] # word: rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dictionary[5234] # rank: word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common words:') \n",
    "print(*count[:5], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Least common words:')\n",
    "print(*count[-5:], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Generate a Training Batch for the Skip-Gram Model\n",
    "\n",
    "In this step, we'll create a function to generate training batches for our skip-gram model. The skip-gram model is a variant of Word2Vec that predicts the context words given a target word. This function will help us prepare the data in the correct format for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer[:] = data[:span]\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, \n",
    "                               num_skips=2, \n",
    "                               skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of self-supervised learning\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Step 4: Build and train a skip-gram model.\n",
    "-----\n",
    "\n",
    "The Skip-Gram model is a type of neural network model used in Natural Language Processing (NLP) to learn distributed representations of words, commonly known as word embeddings. \n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships between them, meaning that words with similar meanings are mapped to similar points in the vector space.\n",
    "\n",
    "### How the Skip-Gram Model Works\n",
    "The goal of the Skip-Gram model is to predict the context words given a target word. Here's how it works:\n",
    "\n",
    "Input: The model takes a single word (the target word) from the text as input.\n",
    "\n",
    "Output: The model is trained to predict words that are likely to appear in the context of the target word within a specified window of words before and after the target word. This context window is typically referred to as the \"skip window.\"\n",
    "\n",
    "For example, if the sentence is \"The cat sat on the mat,\" and the target word is \"sat,\" the model might be asked to predict the words \"The,\" \"cat,\" \"on,\" and \"the,\" which appear around \"sat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = v1.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = v1.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = v1.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            v1.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            v1.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        # Explanation of the meaning of NCE loss:\n",
    "        #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(weights=nce_weights,\n",
    "                         biases=nce_biases,\n",
    "                         labels=train_labels,\n",
    "                         inputs=embed,\n",
    "                         num_sampled=num_sampled,\n",
    "                         num_classes=vocabulary_size))\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = v1.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "          normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "          valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Step 5: Begin training.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_steps = 2_001 #1 #2_001 #100_001\n",
    "\n",
    "with v1.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2_000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 2_000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 4 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log_str = \"Nearest to '%s':\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print()\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/watiing.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Step 6: Visualize the embeddings with t-SNE.\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we use the t-SNE algorithm to reduce the dimensionality of the learned word embeddings for visualization. \n",
    "\n",
    "The top 500 words (based on frequency) are plotted in a 2D space, allowing us to visually inspect the relationships and groupings among words. \n",
    "\n",
    "This visualization is a powerful tool to understand how well the model has learned semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30, \n",
    "            n_components=2, \n",
    "            init='pca', \n",
    "            n_iter=5_000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "\n",
    "n_words_to_visualize = 40\n",
    "\n",
    "for i, label in enumerate(labels[:n_words_to_visualize]):\n",
    "        x, y = low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Let's render and plot more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='../images/tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  #in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And voila!\n",
    "\n",
    "Now you have a visual understanding of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for following this tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
